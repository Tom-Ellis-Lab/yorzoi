{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5481123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clex.benchmark.utils import load_model_for_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0fa14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, _, _ = load_model_for_testing(\n",
    "    base_path=\"/home/workspace/clex/runs/2025-05-06_12-34-12_nanopore_full_debug\",\n",
    "    skip_data=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "082de5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): TargetLengthCrop()\n",
       "  (1): Borzoi(\n",
       "    (conv_dna): ConvDna(\n",
       "      (conv_layer): Conv1d(4, 256, kernel_size=(15,), stride=(1,), padding=same)\n",
       "      (max_pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (_max_pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (res_tower): Sequential(\n",
       "      (0): ConvBlock(\n",
       "        (norm): BatchNorm1d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): GELU(approximate='tanh')\n",
       "        (conv_layer): Conv1d(256, 320, kernel_size=(5,), stride=(1,), padding=same)\n",
       "      )\n",
       "      (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): ConvBlock(\n",
       "        (norm): BatchNorm1d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): GELU(approximate='tanh')\n",
       "        (conv_layer): Conv1d(320, 384, kernel_size=(5,), stride=(1,), padding=same)\n",
       "      )\n",
       "      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): ConvBlock(\n",
       "        (norm): BatchNorm1d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): GELU(approximate='tanh')\n",
       "        (conv_layer): Conv1d(384, 448, kernel_size=(5,), stride=(1,), padding=same)\n",
       "      )\n",
       "    )\n",
       "    (unet1): Sequential(\n",
       "      (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): ConvBlock(\n",
       "        (norm): BatchNorm1d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): GELU(approximate='tanh')\n",
       "        (conv_layer): Conv1d(448, 512, kernel_size=(5,), stride=(1,), padding=same)\n",
       "      )\n",
       "    )\n",
       "    (horizontal_conv0): ConvBlock(\n",
       "      (norm): BatchNorm1d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): GELU(approximate='tanh')\n",
       "      (conv_layer): Conv1d(448, 324, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    )\n",
       "    (horizontal_conv1): ConvBlock(\n",
       "      (norm): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation): GELU(approximate='tanh')\n",
       "      (conv_layer): Conv1d(512, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    )\n",
       "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (transformer): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): FlashAttention(\n",
       "              (mha): MHA(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (inner_attn): FlashSelfAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): FlashCrossAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (5): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): FlashAttention(\n",
       "              (mha): MHA(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (inner_attn): FlashSelfAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): FlashCrossAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (5): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): FlashAttention(\n",
       "              (mha): MHA(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (inner_attn): FlashSelfAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): FlashCrossAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (5): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): FlashAttention(\n",
       "              (mha): MHA(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (inner_attn): FlashSelfAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): FlashCrossAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (5): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): FlashAttention(\n",
       "              (mha): MHA(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (inner_attn): FlashSelfAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): FlashCrossAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (5): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): FlashAttention(\n",
       "              (mha): MHA(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (inner_attn): FlashSelfAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): FlashCrossAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (5): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): FlashAttention(\n",
       "              (mha): MHA(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (inner_attn): FlashSelfAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): FlashCrossAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (5): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): FlashAttention(\n",
       "              (mha): MHA(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (inner_attn): FlashSelfAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): FlashCrossAttention(\n",
       "                  (drop): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (5): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (upsampling_unet1): Sequential(\n",
       "      (0): ConvBlock(\n",
       "        (norm): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): GELU(approximate='tanh')\n",
       "        (conv_layer): Conv1d(512, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      )\n",
       "      (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    )\n",
       "    (separable1): Sequential(\n",
       "      (0): ConvBlock(\n",
       "        (norm): Identity()\n",
       "        (conv_layer): Sequential(\n",
       "          (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=same, groups=512, bias=False)\n",
       "          (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (activation): Identity()\n",
       "      )\n",
       "      (1): Conv1d(512, 324, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (upsampling_unet0): Sequential(\n",
       "      (0): ConvBlock(\n",
       "        (norm): BatchNorm1d(324, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): GELU(approximate='tanh')\n",
       "        (conv_layer): Conv1d(324, 324, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      )\n",
       "      (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    )\n",
       "    (separable0): Sequential(\n",
       "      (0): ConvBlock(\n",
       "        (norm): Identity()\n",
       "        (conv_layer): Sequential(\n",
       "          (0): Conv1d(324, 324, kernel_size=(3,), stride=(1,), padding=same, groups=324, bias=False)\n",
       "          (1): Conv1d(324, 324, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (activation): Identity()\n",
       "      )\n",
       "      (1): Conv1d(324, 324, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (crop): TargetLengthCrop()\n",
       "    (final_joined_convs): Sequential(\n",
       "      (0): ConvBlock(\n",
       "        (norm): BatchNorm1d(324, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): GELU(approximate='tanh')\n",
       "        (conv_layer): Conv1d(324, 162, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      )\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): GELU(approximate='tanh')\n",
       "    )\n",
       "    (human_head): Conv1d(162, 162, kernel_size=(1,), stride=(1,))\n",
       "    (final_softplus): Softplus(beta=1.0, threshold=20.0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from clex.utils import untransform_then_unbin\n",
    "\n",
    "device = \"cuda:0\"\n",
    "output_dir = (\n",
    "    \"/home/workspace/clex/runs/2025-05-06_12-34-12_nanopore_full_debug/eval/DREAM\"\n",
    ")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e576c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from utils import PromoterDataset, plot_tensor_tracks\n",
    "from clex.train import Config\n",
    "\n",
    "# Import our batch decoder\n",
    "from clex.utils import batch_one_hot_decode\n",
    "\n",
    "\n",
    "def predictions_to_dataframe(prediction, exp_val, yfp_start=1, yfp_end=72):\n",
    "    \"\"\"\n",
    "    Convert model predictions, sequences, and expression values into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        prediction (torch.Tensor): Model predictions of shape (batch_size, tracks, seq_len)\n",
    "        seqs (torch.Tensor): Input sequences (one-hot encoded with shape [batch_size, seq_len, 4])\n",
    "        exp_val (torch.Tensor): Expression values\n",
    "        yfp_start (int): Start position of YFP region (inclusive)\n",
    "        yfp_end (int): End position of YFP region (inclusive)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns for sequences, expression values, and track sums\n",
    "    \"\"\"\n",
    "    num_tracks = prediction.shape[1]\n",
    "\n",
    "    # Calculate YFP region sums for each track\n",
    "    yfp_region = prediction[\n",
    "        :, :, yfp_start : yfp_end + 1\n",
    "    ]  # +1 because end is inclusive\n",
    "    yfp_sums = yfp_region.sum(dim=2).cpu().numpy()  # Sum along sequence dimension\n",
    "\n",
    "    # Convert sequences to strings using our batch decoder\n",
    "\n",
    "    # Convert expression values to numpy array\n",
    "    exp_val_np = exp_val.cpu().numpy()\n",
    "\n",
    "    # Create a dictionary to build the DataFrame\n",
    "    data_dict = {\n",
    "        \"expression_value\": exp_val_np.flatten(),  # Ensure it's 1D\n",
    "    }\n",
    "\n",
    "    # Add track sum columns\n",
    "    for i in range(num_tracks):\n",
    "        data_dict[f\"track_{i}_sum\"] = yfp_sums[:, i]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_track_expression_scatter_plots(df, output_dir, region_name=\"YFP\"):\n",
    "    \"\"\"\n",
    "    Create scatter plots of track coverage sums against expression values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing expression values and track sums\n",
    "        output_dir (str): Directory to save the plots\n",
    "        region_name (str): Name of the region being analyzed (for plot titles)\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get list of track columns\n",
    "    track_columns = [col for col in df.columns if col.startswith(\"track_\")]\n",
    "\n",
    "    # Create subplot figure for all tracks\n",
    "    num_tracks = len(track_columns)\n",
    "    fig_combined, axes = plt.subplots(\n",
    "        nrows=(num_tracks + 1) // 2,\n",
    "        ncols=2,\n",
    "        figsize=(14, 3 * ((num_tracks + 1) // 2)),\n",
    "        tight_layout=True,\n",
    "    )\n",
    "    axes = axes.flatten() if num_tracks > 1 else [axes]\n",
    "\n",
    "    # Create individual scatter plots for each track\n",
    "    for i, track_col in enumerate(track_columns):\n",
    "        track_num = track_col.split(\"_\")[1]\n",
    "\n",
    "        # Calculate correlation\n",
    "        correlation, p_value = pearsonr(df[track_col], df[\"expression_value\"])\n",
    "\n",
    "        # Individual plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(df[track_col], df[\"expression_value\"], alpha=0.6)\n",
    "        plt.title(\n",
    "            f\"Track {track_num} {region_name} Coverage vs Expression\\nr = {correlation:.3f}, p = {p_value:.3e}\"\n",
    "        )\n",
    "        plt.xlabel(f\"Track {track_num} {region_name} Coverage Sum\")\n",
    "        plt.ylabel(\"Expression Value\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add best fit line\n",
    "        z = np.polyfit(df[track_col], df[\"expression_value\"], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_range = np.linspace(df[track_col].min(), df[track_col].max(), 100)\n",
    "        plt.plot(x_range, p(x_range), \"r--\", alpha=0.8)\n",
    "\n",
    "        # Save individual plot\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/track_{track_num}_expression_scatter.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Add to combined subplot figure\n",
    "        axes[i].scatter(df[track_col], df[\"expression_value\"], alpha=0.6)\n",
    "        axes[i].set_title(f\"Track {track_num}\\nr = {correlation:.3f}\")\n",
    "        axes[i].set_xlabel(f\"Track {track_num} Coverage Sum\")\n",
    "        axes[i].set_ylabel(\"Expression Value\")\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "        # Add best fit line to combined plot\n",
    "        axes[i].plot(x_range, p(x_range), \"r--\", alpha=0.8)\n",
    "\n",
    "    # Save the combined figure\n",
    "    fig_combined.suptitle(\n",
    "        f\"{region_name} Region Track Coverage vs Expression\", fontsize=16\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    fig_combined.subplots_adjust(top=0.94)  # Make room for the suptitle\n",
    "    plt.savefig(f\"{output_dir}/all_tracks_expression_scatter.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Create a correlation summary plot\n",
    "    correlations = []\n",
    "    p_values = []\n",
    "    track_names = []\n",
    "\n",
    "    for track_col in track_columns:\n",
    "        track_num = track_col.split(\"_\")[1]\n",
    "        corr, p_val = pearsonr(df[track_col], df[\"expression_value\"])\n",
    "        correlations.append(corr)\n",
    "        p_values = p_values\n",
    "        track_names.append(f\"Track {track_num}\")\n",
    "\n",
    "    # Create correlation summary bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(track_names, correlations)\n",
    "    plt.axhline(y=0, color=\"k\", linestyle=\"-\", alpha=0.3)\n",
    "    plt.title(f\"Correlation between {region_name} Track Coverage and Expression\")\n",
    "    plt.ylabel(\"Pearson Correlation\")\n",
    "    plt.ylim(-1, 1)\n",
    "\n",
    "    # Add correlation values on top of bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.05 * (1 if height >= 0 else -1),\n",
    "            f\"{correlations[i]:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\" if height >= 0 else \"top\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/track_correlations_summary.png\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99f07362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_582/2203338445.py:60: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
      "Inference:   0%|          | 0/712 [00:00<?, ?it/s]/home/workspace/clex/clex/models/yorzoi.py:352: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "Inference: 100%|██████████| 712/712 [00:28<00:00, 25.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved complete results to /home/workspace/clex/runs/2025-05-06_12-34-12_nanopore_full_debug/DREAM/output/all_samples_results.csv\n",
      "Generated scatter plots in /home/workspace/clex/runs/2025-05-06_12-34-12_nanopore_full_debug/DREAM/plots\n",
      "Saved track statistics.\n",
      "Saved track-expression correlations.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GPU inference – no sequence caching, no full-tensor caching\n",
    "===========================================================\n",
    "\n",
    "* Runs the model batch-by-batch.\n",
    "* Immediately reduces the prediction tensor to\n",
    "      (batch, tracks) = Σ_{pos ∈ [YFP start, YFP end)} prediction[..., pos]\n",
    "  and discards the big (tracks, seq_len) array.\n",
    "* Builds the final DataFrame from these small per-track sums plus the labels.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# paths & constants\n",
    "# ---------------------------------------------------------------------------\n",
    "promoter_path = (\n",
    "    \"/home/workspace/clex/clex/benchmark/DREAM/\"\n",
    "    \"GSE254493_filtered_test_data_with_MAUDE_expression.txt\"\n",
    ")\n",
    "output_dir = \"/home/workspace/clex/runs/2025-05-06_12-34-12_nanopore_full_debug/DREAM/output\"\n",
    "plots_dir = \"/home/workspace/clex/runs/2025-05-06_12-34-12_nanopore_full_debug/DREAM/plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "DEVICE = device  # defined earlier in your notebook\n",
    "\n",
    "sections = {\"YFP start\": 1, \"YFP end\": 90}\n",
    "yfp_start, yfp_end = sections[\"YFP start\"], sections[\"YFP end\"]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# dataset / loader / model\n",
    "# ---------------------------------------------------------------------------\n",
    "test_dataset = PromoterDataset(promoter_path)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# inference loop – keep only tiny per-track region sums\n",
    "# ---------------------------------------------------------------------------\n",
    "region_sums_accum = []  # list of (batch, tracks) CPU tensors\n",
    "expr_accum = []  # list of (batch, 1)   CPU tensors\n",
    "sample_ids = []  # list of str\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    for b_idx, (seqs, exp_val) in enumerate(tqdm(test_loader, desc=\"Inference\")):\n",
    "        preds = model(seqs.to(DEVICE, non_blocking=True))\n",
    "\n",
    "        # Reduce *inside* the loop to avoid holding the big tensor\n",
    "        region_sum = preds[:, :, yfp_start:yfp_end].sum(dim=2).cpu()  # (B, tracks)\n",
    "\n",
    "        region_sums_accum.append(region_sum)\n",
    "        expr_accum.append(exp_val.cpu())\n",
    "\n",
    "        offset = b_idx * test_loader.batch_size\n",
    "        sample_ids.extend([f\"sample_{offset + j}\" for j in range(seqs.size(0))])\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# build the final DataFrame\n",
    "# ---------------------------------------------------------------------------\n",
    "region_sums_full = torch.cat(region_sums_accum, dim=0).numpy()  # (N, tracks)\n",
    "expr_full = torch.cat(expr_accum, dim=0).squeeze(1).numpy()\n",
    "\n",
    "num_tracks = region_sums_full.shape[1]\n",
    "\n",
    "data = {\n",
    "    \"sample_id\": sample_ids,\n",
    "    \"expression_value\": expr_full,\n",
    "}\n",
    "for t in range(num_tracks):\n",
    "    data[f\"track_{t}\"] = region_sums_full[:, t]\n",
    "\n",
    "final_df = pd.DataFrame(data)\n",
    "\n",
    "csv_path = os.path.join(output_dir, \"all_samples_results.csv\")\n",
    "final_df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved complete results to {csv_path}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# downstream analysis – unchanged\n",
    "# ---------------------------------------------------------------------------\n",
    "create_track_expression_scatter_plots(final_df, plots_dir, region_name=\"YFP\")\n",
    "print(f\"Generated scatter plots in {plots_dir}\")\n",
    "\n",
    "track_cols = [c for c in final_df.columns if c.startswith(\"track_\")]\n",
    "stats_df = final_df[track_cols + [\"expression_value\"]].describe()\n",
    "stats_df.to_csv(os.path.join(output_dir, \"track_statistics.csv\"))\n",
    "print(\"Saved track statistics.\")\n",
    "\n",
    "corr_records = [\n",
    "    {\n",
    "        \"track\": col,\n",
    "        \"correlation_with_expression\": pearsonr(\n",
    "            final_df[col], final_df[\"expression_value\"]\n",
    "        )[0],\n",
    "        \"p_value\": pearsonr(final_df[col], final_df[\"expression_value\"])[1],\n",
    "    }\n",
    "    for col in track_cols\n",
    "]\n",
    "pd.DataFrame(corr_records).to_csv(\n",
    "    os.path.join(output_dir, \"track_expression_correlations.csv\"), index=False\n",
    ")\n",
    "print(\"Saved track-expression correlations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ecfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
